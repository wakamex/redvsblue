You are reviewing a statistical-methods pipeline that compares US economic performance across Democrat vs Republican presidencies.

Please give a rigorous methods review, not style feedback.

Context summary (current implementation):

1) Primary inferential battery (term-party test):
- Permutation p-values for D-R term-level difference.
- Benjamini-Hochberg FDR q-values (`q_bh_fdr`) across tested metrics.
- Tiering currently centered on q + minimum sample thresholds.

2) Additional diagnostics:
- Bootstrap CI bands.
- HAC/Newey-West p-values.
- President-cluster sandwich diagnostics.
- Wild-cluster bootstrap p-values.

3) Stability checks added recently:
- `inference-stability`: wild-cluster p-value sensitivity across RNG seeds and draw-count grids.
- `inference-stability-summary`: per metric classification at 0.05 and 0.10:
  - `robust_significant`
  - `robust_not_significant`
  - `unstable`

4) Publication artifacts:
- Scoreboard now shows q/p/tier and also stability shorthand (`sig`, `ns`, `unstable`) for term-party metrics.
- Narrative template includes per-metric stability tags and unstable-count summary.
- Claims table has publication gating:
  - HAC/sign gate (existing).
  - optional stability gate that downgrades unstable rows; publication-bundle defaults this gate ON.

5) Existing v2 review conclusions (from prior external review round) emphasized:
- Inference hardening defaults first.
- q-based multiplicity-aware reporting should remain central.
- dual-inference diagnostics are useful.
- avoid over-claiming from fragile significance.

Current question:
- Have we drifted too far by operationalizing stability in publication outputs?
- Should stability be primary screening, or secondary guardrail?
- Should we continue adding a small-cluster exact/randomization inference variant beyond wild-cluster bootstrap, or pare back complexity now?

Please answer in this exact structure:

A) Alignment Check vs v2 Direction
- Is current suite still aligned with the v2 direction? (yes/no/mixed)
- 3 concise reasons.

B) Role of Metrics (q vs wild-cluster p vs stability)
- Recommended hierarchy for decision-making (ordered list).
- Explicitly state whether stability should be primary screening.

C) Complexity Budget Decision
Choose one and defend it:
- Option 1: Continue adding methods now (including small-cluster exact/randomization variant).
- Option 2: Pause additions and pare back / simplify.
- Option 3: Keep current suite, only minor refinements.

D) If Option 1, specify exactly what to add
- Name the exact method(s) you recommend (e.g., Fisher randomization inference at cluster level, wild-cluster refinements, etc.).
- Explain why it adds non-overlapping value vs current stack.
- Give integration guidance in a concise implementation plan.

E) If Option 2 or 3, specify what to remove/demote
- Which outputs/columns/gates should be demoted or hidden by default?
- What should remain default for publication-facing claims?

F) Final Recommendation (short)
- 5-8 bullets, ranked by importance.
- Include one explicit “do now” and one explicit “do not do yet”.
