from __future__ import annotations

import argparse
from pathlib import Path

from rb.env import load_dotenv
from rb.ingest import ingest_from_spec
from rb.metrics import compute_term_metrics
from rb.presidents import ensure_presidents
from rb.randomization import run_randomization
from rb.scoreboard import write_scoreboard_md
from rb.site import write_site_json
from rb.validate import validate_all

PRESIDENT_SOURCES = ("congress_legislators", "wikidata")
PRESIDENT_GRANULARITY = ("tenure", "term")


def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(prog="rb", description="Reproducible D-vs-R performance pipeline tooling.")
    sub = p.add_subparsers(dest="cmd", required=True)

    ingest = sub.add_parser("ingest", help="Fetch and cache raw data + write normalized derived tables.")
    ingest.add_argument("--spec", type=Path, default=Path("spec/metrics_v1.yaml"), help="Metric registry spec YAML.")
    ingest.add_argument("--refresh", action="store_true", help="Re-download and write a new raw artifact version.")
    ingest.add_argument(
        "--sources",
        action="append",
        default=[],
        help="Restrict ingestion to these spec source names (repeatable).",
    )
    ingest.add_argument(
        "--series",
        action="append",
        default=[],
        help="Restrict ingestion to these series keys from the spec (repeatable).",
    )
    ingest.add_argument("--dotenv", type=Path, default=Path(".env"), help="Optional .env file to load into env vars.")

    presidents = sub.add_parser("presidents", help="Fetch and cache presidential terms + party labels.")
    presidents.add_argument("--refresh", action="store_true", help="Re-download and write a new raw artifact version.")
    presidents.add_argument(
        "--source",
        choices=PRESIDENT_SOURCES,
        default="congress_legislators",
        help="Source of presidential terms/party labels.",
    )
    presidents.add_argument(
        "--output",
        type=Path,
        default=Path("data/derived/presidents.csv"),
        help="Output CSV for presidential windows.",
    )
    presidents.add_argument(
        "--granularity",
        choices=PRESIDENT_GRANULARITY,
        default="term",
        help="Emit per-president tenure windows (tenure) or constitutional terms (term).",
    )
    presidents.add_argument("--dotenv", type=Path, default=Path(".env"), help="Optional .env file to load into env vars.")

    compute = sub.add_parser("compute", help="Compute term-level metric table + party summaries.")
    compute.add_argument("--spec", type=Path, default=Path("spec/metrics_v1.yaml"), help="Metric registry spec YAML.")
    compute.add_argument("--attribution", type=Path, default=Path("spec/attribution_v1.yaml"), help="Attribution spec YAML.")
    compute.add_argument(
        "--president-source",
        choices=PRESIDENT_SOURCES,
        default="congress_legislators",
        help="If --presidents does not exist, generate it from this source.",
    )
    compute.add_argument(
        "--president-granularity",
        choices=PRESIDENT_GRANULARITY,
        default="term",
        help="If generating --presidents, choose tenure vs term windows.",
    )
    compute.add_argument(
        "--presidents",
        type=Path,
        default=Path("data/derived/presidents.csv"),
        help="Presidents terms CSV (generated by `rb presidents`).",
    )
    compute.add_argument("--output-terms", type=Path, default=Path("reports/term_metrics_v1.csv"), help="Output CSV (term-level).")
    compute.add_argument("--output-party", type=Path, default=Path("reports/party_summary_v1.csv"), help="Output CSV (party-level).")
    compute.add_argument("--dotenv", type=Path, default=Path(".env"), help="Optional .env file to load into env vars.")

    validate = sub.add_parser("validate", help="Run basic validation checks on derived data + reports.")
    validate.add_argument("--presidents", type=Path, default=Path("data/derived/presidents.csv"), help="Presidents windows CSV.")
    validate.add_argument("--term-metrics", type=Path, default=Path("reports/term_metrics_v1.csv"), help="Term metrics CSV (optional; validated if present).")
    validate.add_argument("--party-summary", type=Path, default=Path("reports/party_summary_v1.csv"), help="Party summary CSV (optional; validated if present).")
    validate.add_argument("--spec", type=Path, default=Path("spec/metrics_v1.yaml"), help="Metric registry spec YAML for symmetry checks.")
    validate.add_argument("--dotenv", type=Path, default=Path(".env"), help="Optional .env file to load into env vars.")

    randomization = sub.add_parser("randomization", help="Run permutation/randomization tests with FDR correction.")
    randomization.add_argument("--term-metrics", type=Path, default=Path("reports/term_metrics_v1.csv"), help="Term metrics CSV.")
    randomization.add_argument(
        "--output",
        type=Path,
        default=Path("reports/permutation_party_term_v1.csv"),
        help="Output CSV for term-level D-vs-R permutation test.",
    )
    randomization.add_argument("--permutations", type=int, default=10000, help="Number of random permutations.")
    randomization.add_argument("--bootstrap-samples", type=int, default=2000, help="Number of bootstrap samples for CI estimates.")
    randomization.add_argument("--seed", type=int, default=42, help="RNG seed for reproducibility.")
    randomization.add_argument(
        "--q-threshold",
        type=float,
        default=0.05,
        help="Confirmatory FDR q-value threshold (default 0.05).",
    )
    randomization.add_argument(
        "--min-term-n-obs",
        type=int,
        default=12,
        help="Minimum n_obs for term-level rows to pass minimum sample-size check.",
    )
    randomization.add_argument(
        "--term-block-years",
        type=int,
        default=0,
        help="If >0, shuffle D/R labels within N-year blocks instead of unrestricted. Default 0 (unrestricted, most conservative).",
    )
    randomization.add_argument("--dotenv", type=Path, default=Path(".env"), help="Optional .env file to load into env vars.")

    scoreboard = sub.add_parser("scoreboard", help="Render a simple markdown scoreboard from computed CSVs.")
    scoreboard.add_argument("--party-summary", type=Path, default=Path("reports/party_summary_v1.csv"), help="Party summary CSV.")
    scoreboard.add_argument(
        "--output",
        type=Path,
        default=Path("reports/scoreboard.md"),
        help="Output markdown path.",
    )
    scoreboard.add_argument("--dotenv", type=Path, default=Path(".env"), help="Optional .env file to load into env vars.")

    export_json = sub.add_parser("export-json", help="Export scoreboard data as JSON for the static site.")
    export_json.add_argument("--party-summary", type=Path, default=Path("reports/party_summary_v1.csv"), help="Party summary CSV.")
    export_json.add_argument("--output-dir", type=Path, default=Path("site"), help="Output directory (writes data.json).")
    export_json.add_argument("--dotenv", type=Path, default=Path(".env"), help="Optional .env file to load into env vars.")

    return p.parse_args()


def main() -> int:
    args = _parse_args()
    load_dotenv(args.dotenv, override=False)

    if args.cmd == "ingest":
        ingest_from_spec(
            spec_path=args.spec,
            refresh=bool(args.refresh),
            only_sources=set(args.sources) if args.sources else None,
            only_series=set(args.series) if args.series else None,
        )
        return 0

    if args.cmd == "presidents":
        ensure_presidents(
            refresh=bool(args.refresh),
            source=str(args.source),
            output_csv=args.output,
            granularity=str(args.granularity),
        )
        return 0

    if args.cmd == "compute":
        if not args.presidents.exists():
            ensure_presidents(
                refresh=False,
                source=str(args.president_source),
                output_csv=args.presidents,
                granularity=str(args.president_granularity),
            )
        compute_term_metrics(
            spec_path=args.spec,
            attribution_path=args.attribution,
            presidents_csv=args.presidents,
            output_terms_csv=args.output_terms,
            output_party_csv=args.output_party,
        )
        return 0

    if args.cmd == "validate":
        status, out = validate_all(
            spec_path=args.spec,
            presidents_csv=args.presidents,
            term_metrics_csv=args.term_metrics if args.term_metrics.exists() else None,
            party_summary_csv=args.party_summary if args.party_summary.exists() else None,
        )
        print(out)
        return status

    if args.cmd == "randomization":
        run_randomization(
            term_metrics_csv=args.term_metrics,
            output_csv=args.output,
            permutations=max(0, int(args.permutations)),
            bootstrap_samples=max(0, int(args.bootstrap_samples)),
            seed=int(args.seed),
            term_block_years=max(0, int(args.term_block_years)),
            q_threshold=float(args.q_threshold),
            min_term_n_obs=max(0, int(args.min_term_n_obs)),
        )
        return 0

    if args.cmd == "scoreboard":
        if not args.party_summary.exists():
            raise FileNotFoundError(f"Missing {args.party_summary}; run `rb compute` first.")
        write_scoreboard_md(
            party_summary_csv=args.party_summary,
            out_path=args.output,
        )
        return 0

    if args.cmd == "export-json":
        if not args.party_summary.exists():
            raise FileNotFoundError(f"Missing {args.party_summary}; run `rb compute` first.")
        write_site_json(
            party_summary_csv=args.party_summary,
            output_dir=args.output_dir,
        )
        return 0

    raise RuntimeError(f"unhandled cmd={args.cmd!r}")
